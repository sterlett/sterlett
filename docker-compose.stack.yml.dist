
# Copyright (c) 2020-2021 Pavel Petrov <itnelo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses>.

# Don't use it directly, run bin/configure-env to generate docker-compose.stack.yml based on this template.

version: '3.8'

services:
    app:
        image: '${APP_STACK_IMAGE_NAME}:${APP_STACK_IMAGE_VERSION}'
        networks:
            - back_overlay
        deploy:
            mode: replicated
            replicas: 3
            # for cases when we place multiple replicas to a single node; provides all IP addresses to the haproxy
            endpoint_mode: dnsrr
            placement:
                constraints:
                    - "node.role == worker"
                # distribute containers evenly between all available nodes in the swarm by configured geography
                preferences:
                    -   spread: node.labels.provider_location_machine
            restart_policy:
                condition: on-failure
                delay: 3s
                max_attempts: 3
                # time to ensure container is properly restarted
                window: 30s
            update_config:
                parallelism: 1
                # restarting containers one by one with some time intervals to prevent complete service denial
                # in case when all containers are down simultaneously
                delay: 30s
            resources:
                limits:
                    cpus: '0.5'
                    memory: 512M
                reservations:
                    cpus: '0.25'
                    memory: 256M

    database:
        image: 'mysql:${DATABASE_VERSION}'
        # for compatibility with react/mysql client implementation (no caching_sha2_password support at this time)
        command: '--default-authentication-plugin=mysql_native_password'
        networks:
            - back_overlay
        environment:
            MYSQL_RANDOM_ROOT_PASSWORD: 'non-empty'
            MYSQL_DATABASE: ${DATABASE_NAME}
            MYSQL_USER: ${DATABASE_USER_NAME}
            MYSQL_PASSWORD: ${DATABASE_USER_PASSWORD}
        volumes:
            - database_data:${DATABASE_DATA_DIR}
            - ./docker/dev/database/docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d:ro
        deploy:
            mode: replicated
            replicas: 1
            placement:
                constraints:
                    - "node.role == manager"
            restart_policy:
                condition: any
                delay: 5s
                max_attempts: 5
                window: 60s
            resources:
                limits:
                    cpus: '0.5'
                    memory: 512M
                reservations:
                    cpus: '0.25'
                    memory: 256M

    lighttpd:
        image: '${LIGHTTPD_STACK_IMAGE_NAME}:${LIGHTTPD_STACK_IMAGE_VERSION}'
        networks:
            - back_overlay
        deploy:
            mode: replicated
            replicas: 1
            placement:
                constraints:
                    - "node.role == worker"
            restart_policy:
                condition: on-failure
                delay: 3s
                max_attempts: 3
                window: 15s
            resources:
                limits:
                    cpus: '0.5'
                    memory: 128M
                reservations:
                    cpus: '0.25'
                    memory: 64M

    selenium-hub:
        image: 'selenium/hub@${SELENIUM_HUB_IMAGE_DIGEST}'
        networks:
            - back_overlay
        environment:
            GRID_HUB_PORT: ${SELENIUM_HUB_PORT}
            GRID_MAX_SESSION: 1
            # do not release inactive browser nodes (focusing on session persistence)
            GRID_TIMEOUT: 0
            # do not wait for new sessions, we have to use a persistent one (maintaining a white fingerprint)
            GRID_NEW_SESSION_WAIT_TIMEOUT: 5
        deploy:
            mode: replicated
            replicas: 1
            placement:
                constraints:
                    - "node.role == worker"
            restart_policy:
                condition: on-failure
                delay: 3s
                max_attempts: 3
                window: 30s
            resources:
                limits:
                    cpus: '0.5'
                    memory: 384M
                reservations:
                    cpus: '0.25'
                    memory: 192M

    # todo: profile persistence
    selenium-node-chrome:
        image: '${SNC_STACK_IMAGE_NAME}:${SNC_STACK_IMAGE_VERSION}'
        networks:
            - back_overlay
        environment:
            HUB_HOST: ${SELENIUM_HUB_HOST}
            HUB_PORT: ${SELENIUM_HUB_PORT}
            NODE_MAX_INSTANCES: 1
            NODE_MAX_SESSION: 1
            # virtual display resolution (framebuffer mode only)
            SCREEN_WIDTH: 1706
            SCREEN_HEIGHT: 960
        # using a unique hostname, which will be assigned for the replica, to keep hub-node communications
        # within a single address space (overlay network); hub sends heartbeat signals and commands to its
        # nodes and, therefore, we need to properly resolve IPs in both directions.
        entrypoint: bash -c 'SE_OPTS="-host $${HOSTNAME}" /opt/bin/entry_point.sh'
        deploy:
            mode: replicated
            replicas: 1
            placement:
                constraints:
                    - "node.role == worker"
            restart_policy:
                condition: on-failure
                delay: 3s
                max_attempts: 3
                window: 30s
            resources:
                limits:
                    cpus: '0.5'
                    memory: 384M
                reservations:
                    cpus: '0.25'
                    memory: 192M

    haproxy:
        image: 'haproxy:${HAPROXY_VERSION}-alpine'
        networks:
            - back_overlay
        ports:
            -   published: ${HAPROXY_PORT_EXPOSE}
                target: 80
                protocol: tcp
                # to bypass the ingress routing mesh and route requests directly to the local container
                mode: host
        dns:
            # docker dns service at :53; will be polled by the haproxy, according to "resolvers" directive
            - 127.0.0.11
            # optionally, if we want to preserve access to the worldwide network from the container
            - 8.8.8.8
        volumes:
            - ./docker/dev/haproxy/haproxy.stack.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
        deploy:
            mode: replicated
            replicas: 1
            placement:
                # fixed place, on the specified swarm node; single instance
                constraints:
                    - "node.role == manager"
            restart_policy:
                condition: any
                delay: 5s
                max_attempts: 5
                window: 60s

networks:
    back_overlay:
        driver: overlay
        driver_opts:
            # to encrypt data transfer between nodes; whenever swarm nodes are in different data centers and no other
            # security measures applied
            encrypted: ''
        # to ensure an external service can access this network
        attachable: true

volumes:
    database_data:
        driver: local
        driver_opts:
            type: none
            device: ${PWD}/data/database/sterlett_data
            o: bind
